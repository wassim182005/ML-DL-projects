{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb32ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "========================================\n",
      " ⚡ FAST HYPERPARAMETER TUNING \n",
      "========================================\n",
      "Tuning CatBoost...\n",
      "Tuning LightGBM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "========================================\n",
      " ⚡ FAST HYPERPARAMETER TUNING \n",
      "========================================\n",
      "Tuning CatBoost...\n",
      "Tuning LightGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2026-02-13 08:56:57,278]\u001b[0m Trial 0 failed with parameters: {'learning_rate': 0.05631111873741038, 'max_depth': 7, 'subsample': 0.6966448372594408, 'colsample_bytree': 0.6595334871773919} because of the following error: TypeError(\"XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'\").\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"c:\\Users\\DELL\\Desktop\\DL__SOAI\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\"\u001b[0m, line \u001b[35m206\u001b[0m, in \u001b[35m_run_trial\u001b[0m\n",
      "    value_or_values = func(trial)\n",
      "  File \u001b[35m\"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_14652\\3973061318.py\"\u001b[0m, line \u001b[35m175\u001b[0m, in \u001b[35mobj_xgb\u001b[0m\n",
      "    \u001b[31mmodel.fit\u001b[0m\u001b[1;31m(X_tune_tr_enc[encoded_features], y_tune_tr, eval_set=[(X_tune_va_enc[encoded_features], y_tune_va)], early_stopping_rounds=20, verbose=False)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"c:\\Users\\DELL\\Desktop\\DL__SOAI\\venv\\Lib\\site-packages\\xgboost\\core.py\"\u001b[0m, line \u001b[35m751\u001b[0m, in \u001b[35minner_f\u001b[0m\n",
      "    return func(**kwargs)\n",
      "\u001b[1;35mTypeError\u001b[0m: \u001b[35mXGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'\u001b[0m\n",
      "\u001b[33m[W 2026-02-13 08:56:57,281]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "========================================\n",
      " ⚡ FAST HYPERPARAMETER TUNING \n",
      "========================================\n",
      "Tuning CatBoost...\n",
      "Tuning LightGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2026-02-13 08:56:57,278]\u001b[0m Trial 0 failed with parameters: {'learning_rate': 0.05631111873741038, 'max_depth': 7, 'subsample': 0.6966448372594408, 'colsample_bytree': 0.6595334871773919} because of the following error: TypeError(\"XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'\").\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"c:\\Users\\DELL\\Desktop\\DL__SOAI\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\"\u001b[0m, line \u001b[35m206\u001b[0m, in \u001b[35m_run_trial\u001b[0m\n",
      "    value_or_values = func(trial)\n",
      "  File \u001b[35m\"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_14652\\3973061318.py\"\u001b[0m, line \u001b[35m175\u001b[0m, in \u001b[35mobj_xgb\u001b[0m\n",
      "    \u001b[31mmodel.fit\u001b[0m\u001b[1;31m(X_tune_tr_enc[encoded_features], y_tune_tr, eval_set=[(X_tune_va_enc[encoded_features], y_tune_va)], early_stopping_rounds=20, verbose=False)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"c:\\Users\\DELL\\Desktop\\DL__SOAI\\venv\\Lib\\site-packages\\xgboost\\core.py\"\u001b[0m, line \u001b[35m751\u001b[0m, in \u001b[35minner_f\u001b[0m\n",
      "    return func(**kwargs)\n",
      "\u001b[1;35mTypeError\u001b[0m: \u001b[35mXGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'\u001b[0m\n",
      "\u001b[33m[W 2026-02-13 08:56:57,281]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning XGBoost...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 180\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTuning XGBoost...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    179\u001b[39m study_xgb = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[43mstudy_xgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_xgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m best_xgb = study_xgb.best_params\n\u001b[32m    182\u001b[39m best_xgb.update({\u001b[33m'\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m3000\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.015\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mn_jobs\u001b[39m\u001b[33m'\u001b[39m: -\u001b[32m1\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mverbose\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrandom_state\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m42\u001b[39m})\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\Desktop\\DL__SOAI\\venv\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\Desktop\\DL__SOAI\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:68\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\Desktop\\DL__SOAI\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:165\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\Desktop\\DL__SOAI\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:263\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    256\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    259\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    261\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    262\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\Desktop\\DL__SOAI\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:206\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    208\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    209\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 175\u001b[39m, in \u001b[36mobj_xgb\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    166\u001b[39m params = {\n\u001b[32m    167\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m300\u001b[39m,\n\u001b[32m    168\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m: trial.suggest_float(\u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.03\u001b[39m, \u001b[32m0.15\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mn_jobs\u001b[39m\u001b[33m'\u001b[39m: -\u001b[32m1\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mverbose\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrandom_state\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m42\u001b[39m\n\u001b[32m    173\u001b[39m }\n\u001b[32m    174\u001b[39m model = XGBRegressor(**params)\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tune_tr_enc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mencoded_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tune_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tune_va_enc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mencoded_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tune_va\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.sqrt(mean_squared_error(y_tune_va, model.predict(X_tune_va_enc[encoded_features])))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\Desktop\\DL__SOAI\\venv\\Lib\\site-packages\\xgboost\\core.py:751\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    750\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.optimize import minimize\n",
    "import warnings, gc\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# ==========================================\n",
    "# 1. Data Loading + Q99 Outlier Clipping\n",
    "# ==========================================\n",
    "train = pd.read_csv('/kaggle/input/ai-real-camp-3/train_car.csv')\n",
    "test = pd.read_csv('/kaggle/input/ai-real-camp-3/test_car.csv')\n",
    "\n",
    "# Fix int32 overflow mileage\n",
    "train.loc[train['mileage_km'] > 900_000, 'mileage_km'] = np.nan\n",
    "test.loc[test['mileage_km'] > 900_000, 'mileage_km'] = np.nan\n",
    "\n",
    "# Q99 clipping on price\n",
    "q01_price = train['price'].quantile(0.01)\n",
    "q99_price = train['price'].quantile(0.99)\n",
    "train['price'] = train['price'].clip(q01_price, q99_price)\n",
    "\n",
    "# Q99 clipping on mileage\n",
    "q99_mileage = train['mileage_km'].quantile(0.99)\n",
    "train['mileage_km'] = train['mileage_km'].clip(upper=q99_mileage)\n",
    "test['mileage_km'] = test['mileage_km'].clip(upper=q99_mileage)\n",
    "\n",
    "# Q99 clipping on tax_hp\n",
    "train['tax_hp'] = pd.to_numeric(train['tax_hp'], errors='coerce')\n",
    "test['tax_hp'] = pd.to_numeric(test['tax_hp'], errors='coerce')\n",
    "q99_hp = train['tax_hp'].quantile(0.99)\n",
    "train['tax_hp'] = train['tax_hp'].clip(upper=q99_hp)\n",
    "test['tax_hp'] = test['tax_hp'].clip(upper=q99_hp)\n",
    "\n",
    "print(f\"Train: {train.shape}, Test: {test.shape} — ALL rows kept, Q99 clipped\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Feature Engineering\n",
    "# ==========================================\n",
    "def preprocess(df, ref_df=None):\n",
    "    df = df.copy()\n",
    "    if ref_df is None:\n",
    "        ref_df = df\n",
    "\n",
    "    # Date features\n",
    "    df['listing_date'] = pd.to_datetime(df['listing_date'])\n",
    "    df['listing_year'] = df['listing_date'].dt.year\n",
    "    df['listing_month'] = df['listing_date'].dt.month\n",
    "    df['listing_dayofweek'] = df['listing_date'].dt.dayofweek\n",
    "    df['listing_quarter'] = df['listing_date'].dt.quarter\n",
    "    df['car_age'] = (df['listing_year'] - df['year']).clip(lower=0)\n",
    "\n",
    "    # Brand / submodel\n",
    "    df['model'] = df['model'].astype(str).replace('nan', 'Missing')\n",
    "    df['brand'] = df['model'].str.split(' ').str[0]\n",
    "    df['submodel'] = df['model'].str.split(' ', n=1).str[1].fillna('Missing')\n",
    "\n",
    "    # Transmission & Fuel encoding\n",
    "    trans_map = {'Manuelle': 0, 'Automatique': 1}\n",
    "    df['transmission_enc'] = df['transmission'].map(trans_map).fillna(0)\n",
    "    fuel_map = {'Essence': 0, 'Diesel': 1, 'Hybride': 2, 'Electrique': 3}\n",
    "    df['fuel_enc'] = df['fuel_type'].map(fuel_map).fillna(0)\n",
    "\n",
    "    # CatBoost native categoricals\n",
    "    df['transmission_cat'] = df['transmission'].astype(str)\n",
    "    df['fuel_cat'] = df['fuel_type'].astype(str)\n",
    "\n",
    "    # Imputation: cascade model -> brand -> global median\n",
    "    df['tax_hp'] = pd.to_numeric(df['tax_hp'], errors='coerce')\n",
    "    for col_val in ['tax_hp', 'mileage_km']:\n",
    "        model_med = ref_df.groupby('model')[col_val].median()\n",
    "        brand_med = ref_df.groupby('brand')[col_val].median()\n",
    "        df[col_val] = df[col_val].fillna(df['model'].map(model_med)).fillna(\n",
    "            df['brand'].map(brand_med)).fillna(ref_df[col_val].median())\n",
    "\n",
    "    # Doors imputation\n",
    "    df['doors'] = pd.to_numeric(df['doors'], errors='coerce')\n",
    "    df['doors_missing'] = df['doors'].isna().astype(int)\n",
    "    model_doors = ref_df.groupby('model')['doors'].agg(\n",
    "        lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else np.nan)\n",
    "    brand_doors = ref_df.groupby('brand')['doors'].agg(\n",
    "        lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else np.nan)\n",
    "    df['doors'] = df['doors'].fillna(df['model'].map(model_doors)).fillna(\n",
    "        df['brand'].map(brand_doors)).fillna(5)\n",
    "\n",
    "    # Interaction features\n",
    "    df['mileage_per_year'] = df['mileage_km'] / (df['car_age'] + 1)\n",
    "    df['hp_per_age'] = df['tax_hp'] / (df['car_age'] + 1)\n",
    "    df['log_mileage'] = np.log1p(df['mileage_km'])\n",
    "    df['log_hp'] = np.log1p(df['tax_hp'])\n",
    "    df['year_squared'] = df['year'] ** 2\n",
    "    df['age_squared'] = df['car_age'] ** 2\n",
    "    df['hp_squared'] = df['tax_hp'] ** 2\n",
    "    df['mileage_x_hp'] = df['mileage_km'] * df['tax_hp']\n",
    "    df['mileage_per_hp'] = df['mileage_km'] / (df['tax_hp'] + 1)\n",
    "    df['is_new'] = (df['car_age'] <= 1).astype(int)\n",
    "    df['depreciation_rate'] = df['mileage_km'] / (df['car_age'] + 1) / (df['tax_hp'] + 1)\n",
    "    df['hp_density'] = df['tax_hp'] / (df['doors'] + 1)\n",
    "    df['age_mileage_ratio'] = df['car_age'] / (df['mileage_km'] / 10000 + 1)\n",
    "    df['is_diesel_auto'] = ((df['fuel_enc'] == 1) & (df['transmission_enc'] == 1)).astype(int)\n",
    "    df['wear_index'] = df['mileage_km'] * df['car_age']\n",
    "\n",
    "    # Luxury flag\n",
    "    luxury = ['MERCEDES-BENZ', 'BMW', 'AUDI', 'PORSCHE', 'LAND-ROVER',\n",
    "              'JAGUAR', 'VOLVO', 'MASERATI', 'BENTLEY', 'LEXUS', 'INFINITI']\n",
    "    df['is_luxury'] = df['brand'].isin(luxury).astype(int)\n",
    "\n",
    "    # Frequency encoding\n",
    "    for col in ['brand', 'model', 'submodel']:\n",
    "        freq = ref_df[col].astype(str).value_counts(normalize=True)\n",
    "        df[col + '_freq'] = df[col].astype(str).map(freq).fillna(0)\n",
    "\n",
    "    # Brand-level group stats\n",
    "    for stat_col in ['mileage_km', 'tax_hp']:\n",
    "        brand_stat = ref_df.groupby('brand')[stat_col].median()\n",
    "        df[f'brand_med_{stat_col}'] = df['brand'].map(brand_stat).fillna(ref_df[stat_col].median())\n",
    "\n",
    "    for col in ['model', 'brand', 'submodel']:\n",
    "        df[col] = df[col].astype(str).replace('nan', 'Missing')\n",
    "\n",
    "    return df\n",
    "\n",
    "train_clean = preprocess(train)\n",
    "test_clean = preprocess(test, ref_df=train_clean)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Target Encoding (leak-free)\n",
    "# ==========================================\n",
    "def target_encode(tr, va, te, col, target, smooth=20):\n",
    "    global_mean = target.mean()\n",
    "    agg = target.groupby(tr[col]).agg(['count', 'mean'])\n",
    "    smooth_means = (agg['count'] * agg['mean'] + smooth * global_mean) / (agg['count'] + smooth)\n",
    "    tr_enc = tr[col].map(smooth_means).fillna(global_mean)\n",
    "    va_enc = va[col].map(smooth_means).fillna(global_mean)\n",
    "    te_enc = te[col].map(smooth_means).fillna(global_mean) if te is not None else None\n",
    "    return tr_enc, va_enc, te_enc\n",
    "\n",
    "# ==========================================\n",
    "# Feature Definitions\n",
    "# ==========================================\n",
    "cb_cat_features = ['model', 'transmission_cat', 'fuel_cat']\n",
    "\n",
    "num_features = [\n",
    "    'mileage_km', 'year', 'tax_hp', 'car_age',\n",
    "    'listing_month', 'listing_dayofweek', 'listing_quarter',\n",
    "    'mileage_per_year', 'hp_per_age', 'log_mileage', 'log_hp',\n",
    "    'year_squared', 'age_squared', 'hp_squared',\n",
    "    'mileage_x_hp', 'mileage_per_hp',\n",
    "    'doors', 'doors_missing', 'transmission_enc', 'fuel_enc',\n",
    "    'is_luxury', 'is_new', 'is_diesel_auto',\n",
    "    'depreciation_rate', 'hp_density', 'age_mileage_ratio', 'wear_index',\n",
    "    'brand_freq', 'model_freq', 'submodel_freq',\n",
    "    'brand_med_mileage_km', 'brand_med_tax_hp',\n",
    "]\n",
    "\n",
    "cb_features = num_features + cb_cat_features\n",
    "encoded_features = num_features + ['model_enc', 'brand_enc', 'submodel_enc']\n",
    "\n",
    "y_log = np.log1p(train_clean['price'])\n",
    "y_log.name = 'price_log'\n",
    "\n",
    "print(f\"Features: CB={len(cb_features)}, Encoded={len(encoded_features)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. OPTUNA TUNING (50 trials, all 4 models)\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"  OPTUNA TUNING (50 trials each, 4 models)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "N_TRIALS = 50\n",
    "X_tune_tr, X_tune_va, y_tune_tr, y_tune_va = train_test_split(\n",
    "    train_clean, y_log, test_size=0.2, random_state=42)\n",
    "\n",
    "X_tune_tr_enc, X_tune_va_enc = X_tune_tr.copy(), X_tune_va.copy()\n",
    "for col in ['model', 'brand', 'submodel']:\n",
    "    X_tune_tr_enc[col+'_enc'], X_tune_va_enc[col+'_enc'], _ = target_encode(\n",
    "        X_tune_tr, X_tune_va, None, col, y_tune_tr)\n",
    "\n",
    "# --- CatBoost ---\n",
    "def obj_cb(trial):\n",
    "    params = {\n",
    "        'iterations': 3000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.1, 20, log=True),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 2),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0, 3),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 50),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'loss_function': 'RMSE', 'verbose': 0,\n",
    "        'cat_features': cb_cat_features, 'random_seed': 42,\n",
    "    }\n",
    "    m = CatBoostRegressor(**params)\n",
    "    m.fit(X_tune_tr[cb_features], y_tune_tr,\n",
    "          eval_set=(X_tune_va[cb_features], y_tune_va), early_stopping_rounds=150)\n",
    "    return np.sqrt(mean_squared_error(y_tune_va, m.predict(X_tune_va[cb_features])))\n",
    "\n",
    "print(\"Tuning CatBoost (50 trials)...\")\n",
    "study_cb = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_cb.optimize(obj_cb, n_trials=N_TRIALS)\n",
    "best_cb = study_cb.best_params\n",
    "best_cb.update({'iterations': 10000, 'cat_features': cb_cat_features,\n",
    "                'loss_function': 'RMSE', 'verbose': 0, 'random_seed': 42})\n",
    "print(f\"  Best CB: {study_cb.best_value:.5f}\")\n",
    "\n",
    "# --- LightGBM ---\n",
    "def obj_lgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': 3000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 15, 255),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 100),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 100, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-4, 100, log=True),\n",
    "        'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 1.0),\n",
    "        'verbose': -1, 'random_state': 42, 'n_jobs': -1,\n",
    "    }\n",
    "    m = LGBMRegressor(**params)\n",
    "    m.fit(X_tune_tr_enc[encoded_features], y_tune_tr,\n",
    "          eval_set=[(X_tune_va_enc[encoded_features], y_tune_va)],\n",
    "          callbacks=[__import__('lightgbm').early_stopping(150, verbose=False)])\n",
    "    return np.sqrt(mean_squared_error(y_tune_va, m.predict(X_tune_va_enc[encoded_features])))\n",
    "\n",
    "print(\"Tuning LightGBM (50 trials)...\")\n",
    "study_lgb = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_lgb.optimize(obj_lgb, n_trials=N_TRIALS)\n",
    "best_lgb = study_lgb.best_params\n",
    "best_lgb.update({'n_estimators': 10000, 'verbose': -1, 'random_state': 42, 'n_jobs': -1})\n",
    "print(f\"  Best LGB: {study_lgb.best_value:.5f}\")\n",
    "\n",
    "# --- XGBoost ---\n",
    "def obj_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': 3000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 50),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 100, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-4, 100, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'early_stopping_rounds': 150,\n",
    "        'n_jobs': -1, 'verbose': 0, 'random_state': 42,\n",
    "    }\n",
    "    m = XGBRegressor(**params)\n",
    "    m.fit(X_tune_tr_enc[encoded_features], y_tune_tr,\n",
    "          eval_set=[(X_tune_va_enc[encoded_features], y_tune_va)], verbose=False)\n",
    "    return np.sqrt(mean_squared_error(y_tune_va, m.predict(X_tune_va_enc[encoded_features])))\n",
    "\n",
    "print(\"Tuning XGBoost (50 trials)...\")\n",
    "study_xgb = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_xgb.optimize(obj_xgb, n_trials=N_TRIALS)\n",
    "best_xgb = study_xgb.best_params\n",
    "best_xgb.update({'n_estimators': 10000, 'n_jobs': -1, 'verbose': 0, 'random_state': 42})\n",
    "print(f\"  Best XGB: {study_xgb.best_value:.5f}\")\n",
    "\n",
    "# --- HistGradientBoosting (NEW: Optuna tuned) ---\n",
    "def obj_hgb(trial):\n",
    "    params = {\n",
    "        'max_iter': 3000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 60),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 15, 255),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 0.01, 50, log=True),\n",
    "        'max_bins': trial.suggest_int('max_bins', 64, 255),\n",
    "        'early_stopping': True, 'validation_fraction': 0.15,\n",
    "        'n_iter_no_change': 150, 'random_state': 42,\n",
    "    }\n",
    "    m = HistGradientBoostingRegressor(**params)\n",
    "    m.fit(X_tune_tr_enc[encoded_features], y_tune_tr)\n",
    "    return np.sqrt(mean_squared_error(y_tune_va, m.predict(X_tune_va_enc[encoded_features])))\n",
    "\n",
    "print(\"Tuning HistGradientBoosting (50 trials)...\")\n",
    "study_hgb = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_hgb.optimize(obj_hgb, n_trials=N_TRIALS)\n",
    "best_hgb = study_hgb.best_params\n",
    "best_hgb.update({'max_iter': 8000, 'early_stopping': True,\n",
    "                 'validation_fraction': 0.1, 'n_iter_no_change': 200, 'random_state': 42})\n",
    "print(f\"  Best HGB: {study_hgb.best_value:.5f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. MULTI-SEED STACKING (5 seeds x 7 folds x 4 models)\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"  MULTI-SEED STACKING (5 seeds x 7 folds)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "SEEDS = [42, 123, 2024, 7, 999]\n",
    "N_FOLDS = 7\n",
    "all_seed_preds = []\n",
    "\n",
    "for seed_idx, SEED in enumerate(SEEDS):\n",
    "    print(f\"\\n--- Seed {SEED} ({seed_idx+1}/{len(SEEDS)}) ---\")\n",
    "    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    oof = {k: np.zeros(len(train_clean)) for k in ['cb', 'lgb', 'xgb', 'hgb']}\n",
    "    tst = {k: np.zeros(len(test_clean)) for k in ['cb', 'lgb', 'xgb', 'hgb']}\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(kf.split(train_clean)):\n",
    "        X_tr = train_clean.iloc[tr_idx].copy()\n",
    "        X_va = train_clean.iloc[va_idx].copy()\n",
    "        X_te = test_clean.copy()\n",
    "        y_tr, y_va = y_log.iloc[tr_idx], y_log.iloc[va_idx]\n",
    "\n",
    "        # Target encode inside CV (leak-free)\n",
    "        for col in ['model', 'brand', 'submodel']:\n",
    "            X_tr[col+'_enc'], X_va[col+'_enc'], X_te[col+'_enc'] = target_encode(\n",
    "                X_tr, X_va, X_te, col, y_tr)\n",
    "\n",
    "        # CatBoost\n",
    "        cb = CatBoostRegressor(**{**best_cb, 'random_seed': SEED})\n",
    "        cb.fit(X_tr[cb_features], y_tr,\n",
    "               eval_set=(X_va[cb_features], y_va), early_stopping_rounds=500)\n",
    "        oof['cb'][va_idx] = cb.predict(X_va[cb_features])\n",
    "        tst['cb'] += cb.predict(X_te[cb_features]) / N_FOLDS\n",
    "\n",
    "        # LightGBM\n",
    "        lgb = LGBMRegressor(**{**best_lgb, 'random_state': SEED})\n",
    "        lgb.fit(X_tr[encoded_features], y_tr,\n",
    "                eval_set=[(X_va[encoded_features], y_va)],\n",
    "                callbacks=[__import__('lightgbm').early_stopping(500, verbose=False)])\n",
    "        oof['lgb'][va_idx] = lgb.predict(X_va[encoded_features])\n",
    "        tst['lgb'] += lgb.predict(X_te[encoded_features]) / N_FOLDS\n",
    "\n",
    "        # XGBoost\n",
    "        xgb = XGBRegressor(**{**best_xgb, 'random_state': SEED, 'early_stopping_rounds': 500})\n",
    "        xgb.fit(X_tr[encoded_features], y_tr,\n",
    "                eval_set=[(X_va[encoded_features], y_va)], verbose=False)\n",
    "        oof['xgb'][va_idx] = xgb.predict(X_va[encoded_features])\n",
    "        tst['xgb'] += xgb.predict(X_te[encoded_features]) / N_FOLDS\n",
    "\n",
    "        # HistGradientBoosting (now Optuna-tuned)\n",
    "        hgb = HistGradientBoostingRegressor(**{**best_hgb, 'random_state': SEED})\n",
    "        hgb.fit(X_tr[encoded_features], y_tr)\n",
    "        oof['hgb'][va_idx] = hgb.predict(X_va[encoded_features])\n",
    "        tst['hgb'] += hgb.predict(X_te[encoded_features]) / N_FOLDS\n",
    "\n",
    "        print(f\"  Fold {fold+1}/{N_FOLDS}\")\n",
    "        gc.collect()\n",
    "\n",
    "    # OOF scores\n",
    "    for k in oof:\n",
    "        print(f\"  {k.upper()}: {np.sqrt(mean_squared_error(y_log, oof[k])):.5f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "    # Ridge stacking\n",
    "    X_s_tr = pd.DataFrame(oof)\n",
    "    X_s_te = pd.DataFrame(tst)\n",
    "    meta = Ridge(alpha=10)\n",
    "    meta.fit(X_s_tr, y_log)\n",
    "    meta_rmse = np.sqrt(mean_squared_error(y_log, meta.predict(X_s_tr)))\n",
    "\n",
    "    # Nelder-Mead blend\n",
    "    def opt_w(w):\n",
    "        w = np.abs(w) / np.abs(w).sum()\n",
    "        blend = sum(w[i] * oof[k] for i, k in enumerate(oof))\n",
    "        return np.sqrt(mean_squared_error(y_log, blend))\n",
    "\n",
    "    res = minimize(opt_w, [0.25]*4, method='Nelder-Mead')\n",
    "    ww = np.abs(res.x) / np.abs(res.x).sum()\n",
    "    blend_rmse = res.fun\n",
    "\n",
    "    print(f\"  Ridge: {meta_rmse:.5f} | Blend: {blend_rmse:.5f}\")\n",
    "    print(f\"  Weights: CB={ww[0]:.3f} LGB={ww[1]:.3f} XGB={ww[2]:.3f} HGB={ww[3]:.3f}\")\n",
    "\n",
    "    if blend_rmse < meta_rmse:\n",
    "        seed_pred = sum(ww[i] * tst[k] for i, k in enumerate(tst))\n",
    "    else:\n",
    "        seed_pred = meta.predict(X_s_te)\n",
    "\n",
    "    all_seed_preds.append(np.expm1(seed_pred))\n",
    "\n",
    "# ==========================================\n",
    "# 6. FINAL SUBMISSION\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"  FINAL SUBMISSION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "final_preds = np.mean(all_seed_preds, axis=0)\n",
    "final_preds = np.clip(final_preds, 5000, 5_000_000)\n",
    "\n",
    "submission = pd.DataFrame({'id': test['id'], 'price': final_preds})\n",
    "submission.to_csv('submission_stacking_fast.csv', index=False)\n",
    "print(f\"Saved! Range: {final_preds.min():.0f} - {final_preds.max():.0f}, Mean: {final_preds.mean():.0f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
